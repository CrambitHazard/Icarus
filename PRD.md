Hereâ€™s the **complete, phase-by-phase product blueprint** for building your offline-first, voice-powered personal agent â€” **â€œIcarusâ€**.

---

# ğŸ§­ PHASED ROADMAP â€” *ICARUS ASSISTANT*

| Phase       | Goal                                                                     | Output                     |
| ----------- | ------------------------------------------------------------------------ | -------------------------- |
| **Phase 1** | Core voice loop (STT â†’ LLM â†’ TTS) + File reading                         | MVP voice assistant        |
| **Phase 2** | File search, edit, and summaries + Tool execution + command confirmation | Functional assistant       |
| **Phase 3** | Wake-word trigger + Memory + Logging                                     | Persistent voice assistant |
| **Phase 4** | Screenshot + OCR + Perplexity integration                                | Multimodal agent           |
| **Phase 5** | Web dashboard + plugin system + local fallback LLM                       | Full Jarvis experience     |

---

# ğŸ“„ PHASE 1 PRD â€” *ICARUS Voice Assistant (MVP)*

### ğŸ§  Overview

A lightweight, locally-run assistant that:

* Listens to mic input (press-to-talk or timed)
* Converts speech to text (via Whisper)
* Sends text to OpenRouter LLM for reasoning
* Speaks the response (via OpenVoice)

---

## ğŸ¯ Goals

| Feature      | Spec                                                            |
| ------------ | --------------------------------------------------------------- |
| ğŸ™ï¸ STT      | Use Whisper (base model, local)                                 |
| ğŸ§  LLM       | OpenRouter API (Mixtral, Claude, etc.)                          |
| ğŸ—£ï¸ TTS      | OpenVoice (local, realistic voice)                              |
| ğŸ“ File read | User says: â€œRead this fileâ€ â†’ assistant reads .txt or .md aloud |
| ğŸ” Flow loop | Trigger â†’ Transcribe â†’ Reason â†’ Speak â†’ Idle                    |
| ğŸ“› Logging   | All inputs/outputs stored in timestamped `log.json`             |

---

## ğŸ§ª Functional Requirements

* `icarus` command or button to start listening
* Mic input recorded via `pyaudio` or `speech_recognition`
* Output spoken via OpenVoice TTS
* Ability to read and summarize `.txt`, `.md`, `.pdf` files on command
* Config to choose OpenRouter default model

---

## ğŸ“ FOLDER STRUCTURE â€” *Phase 1*

```
icarus-assistant/
â”œâ”€â”€ main.py                      # Main loop
â”œâ”€â”€ config/
â”‚   â””â”€â”€ openrouter.yaml          # API key + model choice
â”œâ”€â”€ stt/
â”‚   â””â”€â”€ whisper_wrapper.py       # STT using Whisper base
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ openrouter_client.py     # LLM caller
â”œâ”€â”€ tts/
â”‚   â””â”€â”€ openvoice_wrapper.py     # TTS via OpenVoice
â”œâ”€â”€ actions/
â”‚   â””â”€â”€ read_file.py             # Reads and returns .txt/.md
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ audio_handler.py         # Mic handling utilities
â”œâ”€â”€ data/
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ interaction_log.json
â””â”€â”€ README.md
```

---

## âœ… Milestones for Phase 1

1. âœ… Set up Python environment (`venv`, Whisper, OpenVoice, OpenRouter SDK)
2. âœ… Test local STT via Whisper base
3. âœ… Connect OpenRouter and get model response
4. âœ… Send response to TTS
5. âœ… Implement `read_file.py` (with safety checks)
6. âœ… Build `main.py` loop
7. âœ… Test sample run: â€œRead me this fileâ€ â†’ Icarus speaks output

---

# ğŸ—ï¸ FINAL FOLDER STRUCTURE â€” *ICARUS Full Assistant*

```
icarus/
â”œâ”€â”€ main.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ openrouter.yaml
â”‚   â””â”€â”€ preferences.yaml
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ wakeword_listener.py     # "Icarus" detection
â”‚   â”œâ”€â”€ intent_router.py         # Determines tool vs LLM
â”‚   â”œâ”€â”€ task_dispatcher.py       # Runs matched action
â”‚   â””â”€â”€ memory_manager.py        # Memory, logs, sessions
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ openrouter_client.py
â”œâ”€â”€ tts/
â”‚   â””â”€â”€ openvoice_wrapper.py
â”œâ”€â”€ stt/
â”‚   â””â”€â”€ whisper_wrapper.py
â”œâ”€â”€ vision/
â”‚   â”œâ”€â”€ screenshot.py
â”‚   â””â”€â”€ ocr_reader.py
â”œâ”€â”€ web/
â”‚   â””â”€â”€ perplexity_launcher.py
â”œâ”€â”€ actions/
â”‚   â”œâ”€â”€ read_file.py
â”‚   â”œâ”€â”€ summarize_pdf.py
â”‚   â”œâ”€â”€ search_files.py
â”‚   â”œâ”€â”€ edit_text.py
â”‚   â”œâ”€â”€ move_files.py
â”‚   â”œâ”€â”€ launch_app.py
â”‚   â””â”€â”€ system_tools.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ confirm.py               # Safety confirmations
â”‚   â”œâ”€â”€ path_guard.py            # Folder access rules
â”‚   â””â”€â”€ audio_handler.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ memory.sqlite
â”‚   â””â”€â”€ logs/
â”‚       â”œâ”€â”€ interaction_log.json
â”‚       â””â”€â”€ error_log.json
â”œâ”€â”€ interface/
â”‚   â”œâ”€â”€ voice_console.py         # CLI mic/response viewer
â”‚   â””â”€â”€ dashboard_ui.py          # Optional web UI
â””â”€â”€ README.md
```

---

# ğŸ§© Phase Summary Table

| Phase | Key Additions                                        |
| ----- | ---------------------------------------------------- |
| 1     | STT â†’ LLM â†’ TTS, basic file reading                  |
| 2     | Tool routing, file edits, app control, confirmations |
| 3     | Wake-word trigger, memory, action logging            |
| 4     | Screenshot OCR, Perplexity integration               |
| 5     | GUI dashboard, plugin system, local fallback LLM     |

---

# ğŸ§  Want This Ready on Cursor?

I can now generate:

* A `.cursor.json` task planner
* Cursor-ready folder setup with TODOs as docstrings
* Your `.env` + `openrouter.yaml` scaffolding

Let me know what depth you want next:

* Phase 1 implementation plan (step-by-step per file)?
* Prompt schema examples?
* Tool plugins to prioritize?
